{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BY ARJAV JAIN AND ARYAN GUPTA"
      ],
      "metadata": {
        "id": "AWPKZRA1Ohqe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbYYd0BUNUle",
        "outputId": "5f299cca-2788-4274-c5f3-9aada8af04bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import math\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_list = glob.glob(\"/content/drive/MyDrive/corpus/*\")\n",
        "for i in range(len(file_list)):\n",
        "  name = file_list[i].split(\"/\")\n",
        "  file_list[i] = name[-1]\n",
        "\n",
        "def read_document(file_name):\n",
        "  with open('/content/drive/MyDrive/corpus/{}'.format(file_name)) as file:\n",
        "    lines = file.read()\n",
        "  return lines\n",
        "\n",
        "def preprocess_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('[^A-Za-z0-9.]+', ' ',text)\n",
        "  # preprocess text\n",
        "  tokens = word_tokenize(text)\n",
        "    \n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "    \n",
        "  return stemmed_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate term frequency for both query and doc\n",
        "def calculate_tf(tokens):\n",
        "    tf_dict = defaultdict(int)\n",
        "    for token in tokens:\n",
        "        tf_dict[token] += 1\n",
        "    # convert to log(tf) weights\n",
        "    for term, freq in tf_dict.items():\n",
        "        tf_dict[term] = 1 + math.log10(freq)\n",
        "    return tf_dict"
      ],
      "metadata": {
        "id": "gmTBLQ_kZmgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate inverse document frequency for query\n",
        "def calculate_idf(N, df):\n",
        "    return math.log10(N/df)"
      ],
      "metadata": {
        "id": "yIiMbwNGZoyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to build index\n",
        "def build_index(file_list):\n",
        "    inverted_index = defaultdict(list)\n",
        "    doc_lengths = defaultdict(float)\n",
        "    N = len(file_list)\n",
        "\n",
        "    for doc_id, file_name in enumerate(file_list):\n",
        "        text = read_document(file_name)\n",
        "        tokens = preprocess_text(text)\n",
        "        tf_dict = calculate_tf(tokens)\n",
        "        # print(tf_dict)\n",
        "        # update document lengths\n",
        "        length = 0.0\n",
        "        for term, weight in tf_dict.items():\n",
        "            length += weight**2\n",
        "            # add postings to inverted index\n",
        "            inverted_index[term].append((doc_id, weight))\n",
        "        \n",
        "        # update document length dictionary\n",
        "        doc_lengths[doc_id] = math.sqrt(length)\n",
        "        \n",
        "    return inverted_index, doc_lengths, N"
      ],
      "metadata": {
        "id": "rWUIE4HyZqjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate cosine similarity\n",
        "def calculate_cosine_similarity(query_tf_idf, doc_weights, doc_length):\n",
        "    dot_product = 0.0\n",
        "    for term, weight in query_tf_idf.items():\n",
        "        if term in doc_weights:\n",
        "            dot_product += weight * doc_weights[term]\n",
        "    return dot_product / doc_length"
      ],
      "metadata": {
        "id": "yZmI1_MjZsqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to rank documents based on cosine similarity\n",
        "def rank_documents(query, inverted_index, doc_lengths, N):\n",
        "    tokens = preprocess_text(query)\n",
        "    query_tf = calculate_tf(tokens)\n",
        "    query_tf_idf = {}\n",
        "    for term, tf in query_tf.items():\n",
        "        if term in inverted_index:\n",
        "            df = len(inverted_index[term])\n",
        "            idf = calculate_idf(N, df)\n",
        "            query_tf_idf[term] = tf * idf\n",
        "    \n",
        "    scores = defaultdict(float)\n",
        "    for term, weight in query_tf_idf.items():\n",
        "        postings = inverted_index[term]\n",
        "        idf = calculate_idf(N, len(postings))\n",
        "        for doc_id, doc_weight in postings:\n",
        "            tf_idf = doc_weight * idf\n",
        "            scores[doc_id] += tf_idf * weight\n",
        "    \n",
        "    # normalize scores by document lengths\n",
        "    for doc_id, score in scores.items():\n",
        "        scores[doc_id] /= doc_lengths[doc_id]\n",
        "    \n",
        "    # rank documents by cosine similarity\n",
        "    ranked_documents = sorted(scores.items(), key=lambda x: -x[1])\n",
        "    # return up to top 10 documents\n",
        "    return ranked_documents[:10]"
      ],
      "metadata": {
        "id": "XNdWC_MJNlfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, build the inverted index, document lengths, and number of documents\n",
        "inverted_index, doc_lengths, N = build_index(file_list)\n",
        "\n",
        "# Then, prompt the user to enter a query\n",
        "query = input(\"Enter your query: \")\n",
        "\n",
        "# Use the rank_documents function to get the top 10 ranked documents\n",
        "top_documents = rank_documents(query, inverted_index, doc_lengths, N)\n",
        "\n",
        "# Print out the names of the top ranked documents and their scores\n",
        "for doc_id, score in top_documents:\n",
        "    print(f\"{file_list[doc_id]}, {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fdjlFAaNpJH",
        "outputId": "83dafb21-da47-4239-8101-fb6ff7f7b97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query:  Warwickshire, came from an ancient family and was the heiress to  some land\n",
            "shakespeare.txt, 0.5218952557479883\n",
            "google.txt, 0.06346005353536822\n",
            "zomato.txt, 0.06317755812891683\n",
            "levis.txt, 0.05259403722112643\n",
            "Adobe.txt, 0.05127059683761026\n",
            "nike.txt, 0.05025820195108572\n",
            "huawei.txt, 0.036121264905662025\n",
            "Dell.txt, 0.02818648819317776\n",
            "skype.txt, 0.015972125229477746\n",
            "blackberry.txt, 0.015012957067678082\n"
          ]
        }
      ]
    }
  ]
}